{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from time import sleep,ctime\n",
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import multiprocessing as mp\n",
    "import threading\n",
    "from fake_useragent import UserAgent\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from queue import Queue\n",
    "\n",
    "\n",
    "\n",
    "def main(url_list,d,sate): \n",
    "\n",
    "    # 拿取新IP\n",
    "    # r = requests.get(url='http://pubproxy.com/api/proxy?speed=25&https=true')\n",
    "    ip_number = 0\n",
    "    save_number = 0\n",
    "    fail_number = 0\n",
    "    fail_ip = 0\n",
    "    # while r.status_code != 200:\n",
    "    #     if ip_number<3:\t\n",
    "    #         ip_number += 1\n",
    "    #         r = requests.get(url='http://pubproxy.com/api/proxy?speed=25&https=true')\n",
    "    #         time.sleep(5)\n",
    "    #         print('!=200',ip_number)\n",
    "    #     else:\n",
    "    #         break\n",
    "    # try:\n",
    "    #     t_json = r.json()\n",
    "    #     ip = t_json['data'][0]['ipPort']\n",
    "    #     print(str(d)+'拿到ip'+str(ip))\n",
    "    # except Exception as e:\n",
    "    #     print(str(d)+'失敗'+str(e))\n",
    "\n",
    "\n",
    "    ua = UserAgent()\n",
    "    uu = ua.random\n",
    "    opts = Options()\n",
    "    # # opts.add_argument('--headless')  #規避google bug\n",
    "    # # opts.add_argument('--disable-gpu')\n",
    "\n",
    "    ip =[\"18235.103.88.103:61259\",\"157.230.8.128:8080\",\"45.120.112.65:8080\",\"52.38.67.232:3128\",\"213.109.234.4:8080\",\"67.205.145.105:3128\",\"118.174.233.102:55819\",\"195.230.131.210:3128\",\"36.66.217.179:8080\"]\n",
    "    opts.add_argument(\"user-agent={}\".format(uu))  \n",
    "    opts.add_argument('--proxy-server=http://'+str(ip[d]))\n",
    "    driver = webdriver.Chrome('/Users/max/Documents/GitHub/max2019/爬蟲_產品目錄/Hotcar/chromedriver',chrome_options=opts)\n",
    "    count_num = 0\n",
    "    for i in range(2):\n",
    "\n",
    "        while fail_ip>0:\n",
    "            # proxy\n",
    "            get_ip = 'http://pubproxy.com/api/proxy?speed=25&https=true'\n",
    "            r = requests.get(url=get_ip)\n",
    "            t_json = r.json()\n",
    "            ip = t_json['data'][0]['ipPort']\n",
    "            print('-------------------------------------------------',ip)\n",
    "            ua = UserAgent()\n",
    "            uu = ua.random\n",
    "            opts = Options()\n",
    "            opts.add_argument('--proxy-server=http://'+ip)\n",
    "            opts.add_argument(\"user-agent={}\".format(uu))  \n",
    "            print('-------------------------------------------------開啟新driver')\n",
    "            fail_ip=0\n",
    "            driver.close()\n",
    "            driver = webdriver.Chrome('/Users/max/Documents/GitHub/max2019/爬蟲_產品目錄/Hotcar/chromedriver',chrome_options=opts)\n",
    "        try:\n",
    "            print(str(d)+'開始執行crawler'+str(url_list[i]))\n",
    "            driver.get('https://'+url_list[i])\n",
    "            time.sleep(3)\n",
    "\n",
    "            lock.acquire()\n",
    "\n",
    "            link = url_list[i]\n",
    "            car_id = url_list[i].split('?TSEQNO=')[1].split('&')[0]\n",
    "            soup = BeautifulSoup(driver.page_source,\"html.parser\")\n",
    "            tt = soup.select('#main-message > h1 > span')\n",
    "            print(tt)\n",
    "            if tt == '沒有網際網路連線':\n",
    "                fail_ip +=1 \n",
    "                print(tt[0].text)\n",
    "            \n",
    "            text_hotcar = soup.select('.text')\n",
    "            brand = text_hotcar[0].text.strip()\n",
    "            year = text_hotcar[3].text.strip()\n",
    "\n",
    "            add = soup.select('#cartxtin > div.location')[0].text.split('/')[0].strip()\n",
    "            shop = soup.select('#cartxtin > div.location')[0].text.split('/')[1].strip()\n",
    "            money = float(soup.select('#cartxt > div.title.price > span > span')[0].text.strip())*10000\n",
    "            pic = soup.select('.img')[1].get('style').split('url(\"')[1].split('\")')[0]\n",
    "\n",
    "            stock_dic = {\n",
    "                '連結':[link],\n",
    "                '品牌':[brand],\n",
    "                '車型':[car_id],\n",
    "                '年份':[year],\n",
    "                '售價':[money],\n",
    "                '縣市':[add],\n",
    "                '車商':[shop],\n",
    "                '圖片':[pic],\n",
    "            }\n",
    "            \n",
    "            stock_df = pd.DataFrame(stock_dic)\n",
    "            # stock_df.to_csv('/Users/max/Desktop/0'+str(d)+'.csv', index=False, mode='a', header=False)\n",
    "            stock_df.to_csv('/Users/max/Desktop/05.csv', index=False, mode='a', header=False)\n",
    "            driver.close()\n",
    "            print (str(d)+\"完成save時間\")  \n",
    "            save_number += 1          \n",
    "            lock.release()\n",
    "        except Exception as e:\n",
    "            lock.release()\n",
    "            fail_number += 1          \n",
    "            print(str(d)+'失敗'+str(e))   \n",
    "    sate.append(str(d)+',成功'+str(save_number)+',失敗'+str(fail_number))\n",
    "    print('========================================================================'+str(d)+',成功'+str(save_number)+',失敗'+str(fail_number))\n",
    "#     driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/max/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:50: DeprecationWarning: use options instead of chrome_options\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0開始執行crawlerwww.hotcar.com.tw/CWA/CWA060.html?TSEQNO=103004\n",
      "0失敗list index out of range\n",
      "0開始執行crawlerwww.hotcar.com.tw/CWA/CWA060.html?TSEQNO=103563\n",
      "0失敗list index out of range\n",
      "========================================================================0,成功0,失敗2\n"
     ]
    }
   ],
   "source": [
    "lock=threading.Lock()\n",
    "sate = []\n",
    "url = df['頁面'][:2]\n",
    "\n",
    "main(url,0,sate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lock=threading.Lock()\n",
    "\n",
    "#建立DF\n",
    "df_end = pd.DataFrame({},columns=['連結','品牌','車型','年份','售價','縣市','車商','圖片'])\n",
    "df_end.to_csv('/Users/max/Desktop/05.csv', index=False)\n",
    "\n",
    "#讀取URL\n",
    "df = pd.read_csv('/Users/max/Desktop/raw.csv')\n",
    "\n",
    "\n",
    "t_sum = 2\n",
    "url_list = int(len(df['頁面'])/t_sum)\n",
    "t_list=[]\n",
    "sate = []\n",
    "\n",
    "for i in range(t_sum):\n",
    "    url = df['頁面'][url_list*i:url_list*(i+1)].values\n",
    "    t_list.append(threading.Thread(target=main,args=(url,i,sate)))\n",
    "\n",
    "\n",
    "for t in t_list:\n",
    "    t.start()\n",
    "    time.sleep(7)\n",
    "    print('開始執行:'+str(t))\n",
    "\n",
    "for t in t_list:\n",
    "    t.join()\n",
    "print(sate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main-message > h1 > span\n",
    "\n",
    "無法連上這個網站\n",
    "\n",
    "\n",
    "list index out of range"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
